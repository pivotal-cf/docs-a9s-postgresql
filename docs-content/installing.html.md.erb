---
title: Installing and Configuring the a9s PostgreSQL for PCF
owner: Partners
---
<style>
    .note.warning {
        background-color: #fdd;
        border-color: #fbb
    }
    .note.warning:before {
        color: #f99;
     }
</style>

This topic describes how to install and configure a9s PostgreSQL for Pivotal Cloud Foundry (PCF).


##<a id='prerequisites'></a> Prerequisites

Before installing a9s PostgreSQL for PCF, ensure that you have installed and configured the products and tiles listed in [Requirements](./index.html#reqs).

This comprises the **a9s Consul DNS for PCF** tile and the **a9s BOSH for PCF** tile.

In short, the a9s Consul DNS for PCF tile is required because it is responsible for providing an
internal DNS system to the other a9s service tiles, the a9s PostgreSQL for PCF
included. You can find further information about this tile in [its own documentation](https://docs.pivotal.io/partners/a9s-consul-dns/usecase.html).

As for the a9s BOSH for PCF tile, it is required because it is used by the a9s service tiles,
for the provisioning and lifecycle managment of their vitual machine. Further
information on this tile can be found in [its own documentation](https://docs.pivotal.io/partners/a9s-bosh/usecase.html).

##<a id='install'></a> Install a9s PostgreSQL for PCF

To install a9s PostgreSQL for PCF, perform the following steps:

1. Download the product file from the [Pivotal Network](https://network.pivotal.io/products/a9s-postgresql).

2. Navigate to the Ops Manager Installation Dashboard and click **Import a Product** to upload the product file.

3. Click **Add** or **+**(depending on the PCF version) next to the uploaded a9s PostgreSQL for PCF tile in the Ops Manager
**Available Products** view to add it to your staging area.

4. Click the new tile, then click **Settings** to review the configuration settings for each tab.
	See the following sections for more information on how to configure the properties on each side-tab:
	* [Assign AZs and Networks](#azs)
	* [Configure BOSH Errands](#errands)
	* [Configure Resources](#resource-config)
	* [Configure Stemcells](#stemcell)

5. Click **Apply Changes** to deploy the service.

After you install the a9s PostgreSQL for PCF, it is available in the list of marketplace services. For more information, see [Using a9s MongoDB for PCF](./using.html).


###<a id="azs"></a>Assign AZs and Networks

Use **Assign AZs and Networks** to configure the location of service instance jobs. You should configure your network to balance cluster jobs between as many AZs as possible.

<%= image_tag("az-and-networks.png") %>


###<a id='errands'></a>Configure Errands

Use **Errands** to configure the lifecycle errands that run when you install or uninstall a9s PostgreSQL for PCF. The list below describes each errand. For more information, see [Understanding Lifecycle Errands](https://docs.pivotal.io/tiledev/tile-errands.html).

<%= image_tag("a9s-postgresql-errands.png") %>


#### Post-Deploy Errands

 * **Run BOSH Configurator**:
  This errand configures a9s BOSH for PCF so that it can provision PostgreSQL
  service instances. When enabled, this errand uploads the required releases in
  the a9s BOSH Director. Disabling this errand may speed up the deployment of
  all tiles.

	<p class="note"><strong>Note</strong>: To ensure your configuration remains up-to-date, disable this errand only when necessary.</p>

 * **Run Template Uploader**:
	This errand is required to configure generic components that are included in a9s PostgreSQL for PCF with MPostgreSQLongoDB  configurations.
	Disabling this errand may speed up the deployment of all tiles.

	<p class="note"><strong>Note</strong>: To ensure your configuration remains up-to-date, disable this errand only when necessary.</p>

 * **Run Broker Registrar**:
	This errand registers the a9s PostgreSQL for PCF service broker
	at the Elastic Runtime. This makes the service available in the Elastic Runtime
	marketplace.

 * **Run Smoke Tests**:
 This errand runs a series of basic tests against your a9s PostgreSQL for PCF
 installation to ensure that it is configured properly. Those tests may
 take a while, probably over 30 minutes.


#### Pre-Delete Errands

 * **Delete all a9s PostgreSQL instances**:
	This errand deletes all a9s PostgreSQL instances which were created with `cf create-service` by PCF end users.
	<p class="note warning"><strong>Warning</strong>: This is an absolutly destructive task and can't be undone. All VMs of the service instances will be deleted irrecoverably. In the case a service instance is bound to an app or service keys are existing, the errand will fail.</p>

 * **Run Broker Deregistrar**:
	This will deregister the a9s PostgreSQL for PCF service broker in Elastic Runtime. This errand removes the a9s PostgreSQL service from the Elastic Runtime marketplace.


###<a id='resource-config'></a> Configure Resources

You can configure the dimensions of the VMs that host the a9s PostgreSQL for PCF components.
This configuration does not cover the dedicated PostgreSQL instances that run on VMs provided by the a9s BOSH tile for PCF.
For more information, see [Configure Service Plan VMs](#configure-service-instance).

<p class="note"><strong>Note</strong>: Pivotal recommends using a VM Type of large or greater to support the tile compilation.</p>

<%= image_tag("a9s-postgresql-resource-config.png") %>


###<a id="stemcell"></a>Configure Stemcells

If you want to use a stemcell other than the default, click **Stemcell**, and then click **Import Stemcell** to upload the stemcell.

<%= image_tag("a9s-postgresql-stemcells.png") %>


##<a id="configure-service-instance"></a> Configure Service Plans

The sizes and types of VMs used to provision service instances of a given service plan are specified in the **Cloud Config** pane of the a9s BOSH for PCF tile. See the <em>Cloud Config</em> section of the <a href="http://docs.pivotal.io/partners/a9s-bosh/installing.html">a9s BOSH for PCF installation documentation</a> that corresponds to your IaaS.</p>

Each service plan is defined in the following fields:

| This plan... | Uses these a9s BOSH for PCF settings to determine VM and disk size |
|------|-------|
|postgresql-single-small| Cloud properties for the VM Type Small<br /> Cloud properties for the Disk Type Small|
|postgresql-cluster-small| Cloud properties for the VM Type Small<br /> Cloud properties for the Disk Type Small|
|postgresql-single-big| Cloud properties for the VM Type Large<br /> Cloud properties for the Disk Type Big|
|postgresql-cluster-big| Cloud properties for the VM Type Large<br /> Cloud properties for the Disk Type Big|


##<a id='deployment_updater'></a>Update Existing Service Instances

After updating the a9s PostgreSQL for PCF tile, run the Deployment Updater errand to update all provisioned service instances to the new service and stemcell versions.

<p class="note"><strong>Note</strong>: anynines recommends running the <a href="./installing.html#errands" target="_blank">smoke tests</a> before running this errand.</p>

<p class="note"><strong>Note</strong>: To run the errand, you must know how to use the Ops Manager Director. For more information, see <a href="http://docs.pivotal.io/pivotalcf/1-9/customizing/trouble-advanced.html#prepare" target="_blank">Advanced Troubleshooting with the BOSH CLI</a>.</p>

1. SSH into the Ops Manager VM:
<pre class="terminal">
$ ssh ubuntu@OPS-MANAGER-FQDN
Password: ***********
</pre>

2. Target the BOSH Director:
<pre class="terminal">
$ bosh --ca-cert /var/tempest/workspaces/default/root\_ca\_certificate target OPS-MANAGER-DIRECTOR-IP
Target set to DIRECTOR\_UUID
</pre>

3. Log in to the BOSH Director:
<pre class="terminal">
$ bosh login
Your username: director
Enter password: DIRECTOR\_CREDENTIAL
Logged in as 'director'
</pre>

4. Run `bosh deployments` and record the name of the a9s PostgreSQL for PCF deployment from the **Name** column:
<pre class="terminal">
$ bosh deployments
Acting as user 'director' on 'p-bosh'
+---------------------------------------------+---------------------------------+-------------
| Name                                        | Release(s)                      | Stemcell(s)
+---------------------------------------------+---------------------------------+-------------
| a9s-bosh-a40c3b10d101faca4fbc               | bosh-aws-cpi/53                 | bosh-vsphere
| ...
+---------------------------------------------+---------------------------------+-------------
| a9s-consul-dns-3f3a49fb941f8c0874cb         | consul/9                        | bosh-vsphere
| ...
+---------------------------------------------+---------------------------------+-------------
| a9s-postgresql-service-94443f82b52dcf333bbd | bosh-configurator-postgresql/3  | bosh-vsphere
| ...
+---------------------------------------------+---------------------------------+-------------
</pre>

5. Switch the CLI to operate on the a9s PostgreSQL for PCF deployment by running `bosh deployment /var/tempest/workspaces/default/deployments/DEPLOYMENT-NAME.yml` with the `a9s-postgresql-service-...` name from the previous step:
<pre class="terminal">
$ bosh deployment /var/tempest/workspaces/default/deployments/a9s-postgresql-service-94443f82b52dcf333bbd.yml
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                               Dload  Upload   Total   Spent    Left  Speed
100   352  100   352    0     0  12014      0 --:--:-- --:--:-- --:--:-- 12137
Deployment set to '/var/tempest/workspaces/default/deployments/a9s-postgresql-service-94443f82b52dcf333bbd.yml'</pre>

6. Run `bosh run errand deployment-updater` to force the update for all existing a9s PostgreSQL for PCF instances:
<pre class="terminal">
$ bosh run errand deployment-updater
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                               Dload  Upload   Total   Spent    Left  Speed
100   352  100   352    0     0  26378      0 --:--:-- --:--:-- --:--:-- 27076
Acting as user 'director' on deployment 'a9s-postgresql-service-94443f82b52dcf333bbd' on 'p-bosh'
&nbsp;
Director task 6533
Started preparing deployment > Preparing deployment. Done (00:00:01)
&nbsp;
...
&nbsp;
Errand 'deployment-updater' completed successfully (exit code 0)
</pre>
<p class="note"><strong>Note</strong>: Depending on the number of service instances, the Deployment Updater may take significant time to finish running. BOSH shows no logging output while the errand runs.</p>

##<a id='interacting-with-the-backup-manager'></a> Interacting with the Backup Manager

For the moment, only PCF administrators can interact with the Backup Manager
endpoints to trigger immediate backups and restore backups. We plan to extend this
function to PCF users as well in the near future.

To perform backups and restores, you need to go in the a9s MongoDB for PCF tile,
find out the IP of the one of the Backup Manager instances in the **Status** tab and the
credentials of the service in the **Credentials** under the *backup-manager* job
and the name *Backup Manager Credentials*

<%= image_tag("backup-manager-status-tab.png", width: 320) %>
<%= image_tag("backup-manager-credentials.png", width: 320) %>

Once you have this information, you can trigger a backup on all instances by
calling the **/backup\_agent/backup\_all** endpoint:

```
$ curl backup:53cr3t@172.28.7.64:3000/backup_agent/backup_all -d {}
```

or on a given instance with its GUID by calling the **/backup\_agent/backup**
endpoint:

```
$ curl backup:53cr3t@172.28.7.64:3000/backup_agent/backup -d "instance_guid=1c16933a-892f-4fe0-b968-ea0bf90246c9"
```

Before performing a restore, you will need to find the ID of the Backup and the
ID of the instance. You can do so by listing the instances:

```
$ curl backup:53cr3t@172.28.7.64:3000/instances
```

This command outputs an array composed of objects like this one:

```
{
  "restores": [],
  "backups": [
    {
      "backup_agent_task": {
        "updated_at": "2017-05-19T09:01:31.389Z",
        "created_at": "2017-05-19T09:01:16.064Z",
        "status": "done",
        "task_id": 5,
        "id": 5
      },
      "instance_id": 1,
      "id": 5
    }
  ],
  "instance_id": "1c16933a-892f-4fe0-b968-ea0bf90246c9",
  "id": 1
}
```

You can see there backups and restores that have been performed in the past for
the instance with the instance_id "1c16933a-892f-4fe0-b968-ea0bf90246c9". This
ID corresponds to the instance GUID from Pivotal Cloud Foundry.

Then you can restore your backup by calling the **/backup\_agent/restores**
endpoint with the backup\_id and instance\_id as data.

```
$ curl backup:53cr3t@172.28.7.64:3000/backup_agent/restores -d "backup_id=5" -d "instance_id=1"
```
